# -*- coding: utf-8 -*-
"""End-to-End Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0B4aA5yFGpQx1UWRZUGVZa0pvLU9FVkJXQWdPa0ZvdUROTG40

# Kaggle Titanic
## Logistic Regression with Python


For this lecture we will be working with the [Titanic Data Set from Kaggle](https://www.kaggle.com/c/titanic). This is a very famous dataset.

# Step - 1 : Frame The Problem
"""

ls -l

a = 5

b = 8

a*b

a = 5

b = 8

a*b

a*b

"""The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.

# Step - 2 : Obtain the Data

## Import Libraries
"""

!pip install missingno

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as ms
# %matplotlib inline

!ls -l

"""Pandas provides two important data types with in built functions to be able to provide extensive capability to handle the data.The datatypes include Series and DataFrames.

Pandas provides ways to read or get the data from various sources like read_csv,read_excel,read_html etc.The data is read and stored in the form of DataFrames.
"""

!wget https://www.dropbox.com/s/8grgwn4b6y25frw/titanic.csv

!ls -l

data = pd.read_csv('titanic.csv')

data.head(3)

#to get the last 5 entries of the data
data.tail()

type(data)

data.shape

data.info()

a = data['Age']

a

type(a)

type(data)

data.info()

data.describe()

"""## Accessing individual data in the data frame

### Working with Columns

since each dataframe is a collection of series if we access a single column we get a series object
"""

data['Cabin'].value_counts()

y = data[['Cabin','Parch']].head()

type(y)

y.head()

data.info() #New_parch is added to the set of columns in the data frames



"""# Step - 3 : Analyse the Data"""

ms.matrix(data)



data.info()

"""We can observe that there are missing values in 'Age', 'Cabin' and 'Embarked'. Let's continue."""

data.info()

"""Visualization of data with Seaborn"""

sns.jointplot(x='Fare',y='Age',data=data)

sns.distplot(data['Fare'])

sns.heatmap(data.corr(),cmap='coolwarm')
plt.title('data.corr()')

sns.swarmplot
sns.swarmplot(x='Pclass',y='Age',data=data,palette='Set2')

sns.set_style('whitegrid')
sns.countplot(x='Survived',data=data,palette='RdBu_r')

sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Sex',data=data,palette='RdBu_r')

sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Pclass',data = data,palette='rainbow')

data['Age'].hist(bins = 30, color = 'darkred', alpha = 0.8)

sns.countplot(x = 'SibSp', data = data)

data['Fare'].hist(color = 'green', bins = 40, figsize = (8,3))

"""#### What do you observe from the above charts?

# Step - 4 : Feature Engineering

## Feature Engineering

We want to fill the missing values of the age in the dataset with the average age value for each of the classes. This is called data imputation.
"""

data.info()

plt.figure(figsize=(12, 7))
sns.boxplot(x='Pclass',y='Age',data=data,palette='winter')

data['Cabin'].value_counts()

"""The average age for each of the classes are estimated to be as follows:
  
  * For **Class 1** - The median age is 37
  * For **Class 2** - The median age is 29
  * For **Class 3** - The median age is 24
  
Let's impute these values into the age column.
"""

def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):
        # Class-1
        if Pclass == 1:
            return 37
        # Class-2 
        elif Pclass == 2:
            return 29
        # Class-3
        else:
            return 24

    else:
        return Age

"""Applying the function."""

data['Age'] = data[['Age','Pclass']].apply(impute_age,axis=1)

"""Now let's visualize the missing values."""



ms.matrix(data)

"""The Age column is imputed sucessfully.

Let's drop the Cabin column and the row in the Embarked that is NaN.
"""

data.drop('Cabin', axis = 1,inplace=True)

data.head()

data.dropna(inplace = True)

data["Embarked"].value_counts()

ms.matrix(data)

data.info()

"""## Converting Categorical Features 

We'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.
"""

data['Sex'].value_counts()

data['Embarked'].value_counts()

sex = pd.get_dummies(data['Sex'],drop_first=True)
embark = pd.get_dummies(data['Embarked'],drop_first=True)
embark.head()

sex.head()

data.head()

old_data = data.copy()
data.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)
data.head()

#data.drop(['PassengerId'],axis=1,inplace=True)

#data.info()

data = pd.concat([data,sex,embark],axis=1)

data.head()

data.info()

data.describe()

"""# Step - 5 : Model Selection

## Building a Logistic Regression model
"""

from sklearn.model_selection import train_test_split

X = data.drop('Survived',axis=1)
y = data['Survived']

X_train, X_test, y_train, y_test = train_test_split(data.drop('Survived',axis=1), 
                                                    data['Survived'], test_size=0.30, 
                                                    random_state=42)

len(y_test)

268/889

from sklearn.linear_model import LogisticRegression

# Build the Model.
logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)

logmodel.coef_

predict =  logmodel.predict(X_test)
predict[:5]

y_test[:5]

"""Let's move on to evaluate our model.

# Step - 6 : Evaluation

## Evaluation

We can check precision, recall, f1 - score using classification report!

#### Confusion Matrix
"""

from sklearn.metrics import confusion_matrix, classification_report

"""## Confusion Matrix

True positive   |	False positive,  
________________|________________  
                |                  
False negative  |	True negative
"""

print(confusion_matrix(y_test, predict))

"""#### Precision Score

The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.

The best value is 1 and the worst value is 0.
"""

from sklearn.metrics import precision_score

print(precision_score(y_test,predict))

"""#### Recall score

The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.

The best value is 1 and the worst value is 0.
"""

from sklearn.metrics import recall_score

print(recall_score(y_test,predict))

"""#### f1_score

The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:
                F1 = 2 \* (precision \* recall) / (precision + recall)
"""

from sklearn.metrics import f1_score

print(f1_score(y_test,predict))

"""To get all the above metrics at one go, use the following function:"""

from sklearn.metrics import classification_report

print(classification_report(y_test,predict))



"""# Step - 7 : Predict on New Cases

## Prediction on Test Data From Kaggle

create an account on www.kaggle.com
"""

!wget https://www.dropbox.com/s/6hyaidkuyfsv45u/production.csv
prod_data=pd.read_csv('production.csv')

prod_data.info()

prod_data.head()

ms.matrix(prod_data)

"""### Data Cleaning

There are inconsistencies in test data.We can use the same graph functions that are used to visualize the train data for test data as well.We use the same data cleaning techniques like removing the cabin column and applying impute_age function on age column on test data.  
But we cannot remove any rows because kaggle wants same number of rows in submission csv also. So we fill the missing values in fare with mean.
"""

prod_data['Age'] = prod_data[['Age','Pclass']].apply(impute_age,axis=1)

ms.matrix(prod_data)

prod_data.drop('Cabin', axis = 1, inplace= True)

ms.matrix(prod_data)

prod_data.fillna(prod_data['Fare'].mean(),inplace=True)

prod_data.info()

ms.matrix(prod_data)

sex = pd.get_dummies(prod_data['Sex'], drop_first=True)
embark = pd.get_dummies(prod_data['Embarked'], drop_first=True)

prod_data.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)

prod_data = pd.concat([prod_data,sex,embark],axis=1)

prod_data.head()

predict1=logmodel.predict(prod_data)

predict1

df1=pd.DataFrame(predict1,columns=['Survived'])

df2=pd.DataFrame(prod_data['PassengerId'],columns=['PassengerId'])

df2.head()

result = pd.concat([df2,df1],axis=1)
result.head()

result.to_csv('result.csv',index=False)

from google.colab import files
files.download('result.csv')

!ls -l



"""Submit the file to https://www.kaggle.com/c/titanic

---
                                                     THE END
"""