# -*- coding: utf-8 -*-
"""Copy of Twitter_Sentiment_Analysis (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Db5O--PGhCE1bCE-38kSl7FuuTUtQz78

# Twitter Sentiment Analysis

# 01 :Frame the Problem

#### Problem Statement Link :  https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/

# 02 :Obtain Data

### Import Statements
"""

!mkdir twitter
# %cd twitter
!ls

!pip install missingno
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as ms
# % matplotlib inline

"""### Reading the Train Data"""

!wget https://www.dropbox.com/s/p8fq1p6wan2g89a/train.csv

!ls -l

train = pd.read_csv('train.csv')
train.info()

"""# 03 : Analyze Data"""

train.head()

train['label'].value_counts()

train[train['label']==1]['tweet'].head()

"""## Label types
-   0 : Normal
-   1 : Hate

# 05 : Model Selection ( 1st Iteration)

## RandomForest without Preprocessing of Text Data
"""

#Building the model without preprocessing of data
unprocessed_data = pd.read_csv('train.csv')

from sklearn.model_selection import train_test_split


#splitting the data into random train and test subsets
X_train, X_test, y_train, y_test = train_test_split(unprocessed_data["tweet"],
                                                        unprocessed_data["label"], 
                                                    test_size = 0.2, random_state = 42)

# Sequentialization of tasks
from sklearn.pipeline import Pipeline

#generating ngrams and tokens and Bagging
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

from sklearn.ensemble import RandomForestClassifier

text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),
                      ('tfidf', TfidfTransformer()),
                     ('clf', RandomForestClassifier(n_estimators=50)),])

model = text_clf.fit(X_train,y_train)

predicted = model.predict(X_test)

from sklearn.metrics import precision_score,recall_score,f1_score, accuracy_score, confusion_matrix

confusion_matrix(y_test,predicted)

accuracy_score(y_test,predicted)

precision_score(y_test,predicted)

recall_score(y_test,predicted)

f1_score(y_test,predicted)

"""# 04 and 05 : Feature Engineering and Model Selection (2nd Iteration)

Preprocessing of Text data is very important for Textual Analysis. Tokenization, Feature Extraction (Vectorization) are the most important techniques in Scikit-Learn. 
The text must be parsed to extract words, called tokenization. Then the words need to be encoded as integers or floating point values for use as input to a machine learning algorithm, called feature extraction (or vectorization).
"""

#regular expression 
import re 

#regular expression for the removal of name tags and the emoticons from tweets.
def process_tweet(tweet):
    return " ".join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])", " ",tweet.lower()).split())

#Dropping of columns from pd
def drop_features(features,data):
    data.drop(features,inplace=True,axis=1)

#Applying the Process_tweet function to the given Train Data
train['processed_tweets'] = train['tweet'].apply(process_tweet)

train.head()

train[train['label']==1].head(20)

drop_features(['id','tweet'],train)

train.head()

#splitting the data into random train and test subsets
x_train, x_test, y_train, y_test = train_test_split(train["processed_tweets"],train["label"],
                                                    test_size = 0.2, random_state = 42)

"""Pipeline : Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit."""

text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),
                      ('tfidf', TfidfTransformer()),
                     ('clf', RandomForestClassifier(n_estimators=200)),])
text = text_clf.fit(x_train,y_train)

predicted = text.predict(x_test)

from sklearn.metrics import confusion_matrix, classification_report,precision_score

cm_m = confusion_matrix(y_test,predicted)
cm_m

TN, FP = cm_m[0]
FN, TP = cm_m[1]

TP

float(TN+TP)/(TN+TP+FN+FP)

p = TP/(TP+FP)
p

precision_score(y_test,predicted)

r = TP/(FN+TP)
r

recall_score(y_test,predicted)

f1 = 2*p*r/(p+r)
f1

f1_score(y_test,predicted)

"""# 04 and 05 : Feature Engineering and Model Selection (3rd Iteration)"""

count_vect = CountVectorizer(stop_words='english',ngram_range=(1,2),analyzer='word')
transformer = TfidfTransformer(norm='l2',sublinear_tf=True)

#splitting the data into random train and test subsets
x_train, x_test, y_train, y_test = train_test_split(train["processed_tweets"],train["label"],
                                                    test_size = 0.2, random_state = 42)

x_train_counts = count_vect.fit_transform(x_train)
x_train_tfidf = transformer.fit_transform(x_train_counts)
x_test_counts = count_vect.transform(x_test)
x_test_tfidf = transformer.transform(x_test_counts)

print(x_train_counts.shape)
print(x_train_tfidf.shape)
print(x_test_counts.shape)
print(x_test_tfidf.shape)

from sklearn.linear_model import SGDClassifier

model = SGDClassifier(loss="modified_huber", penalty="l1")
model.fit(x_train_tfidf,y_train)
predictions = model.predict(x_test_tfidf)

f1_score(y_test,predictions)

recall_score(y_test,predictions)

precision_score(y_test,predictions)

f1_score(y_test,predictions)

"""# 05 : Model Selection"""

#different classification modesls being used
from sklearn.svm import LinearSVC

model_svc = LinearSVC(C=2.0,max_iter=500,tol=0.0001,loss ='hinge')
model_svc.fit(x_train_counts,y_train)

predict_svc = model_svc.predict(x_test_counts)

f1_score(y_test,predict_svc)

recall_score(y_test,predict_svc)

"""# 06 : Tune the Model"""

#optimizing parameters
from sklearn.model_selection import GridSearchCV


params = {"tfidf__ngram_range": [(1, 2), (1,3)],
          "svc__C": [.01, .1, 1, 10, 100]}

clf = Pipeline([("tfidf", TfidfVectorizer(sublinear_tf=True)),
                ("svc", LinearSVC(loss='hinge'))])

gs = GridSearchCV(clf, params, verbose=2, n_jobs=-1)
gs.fit(x_train,y_train)
print("Best Estimator = ", gs.best_estimator_)
print("Best Score = ",gs.best_score_)

predicted = gs.predict(x_test)

predicted

f1_score(y_test,predicted)

recall_score(y_test,predicted)

precision_score(y_test,predicted)

"""# 07 : Predict on new cases"""

!wget https://www.dropbox.com/s/as2y6lpjsh6284l/test.csv

submission = pd.read_csv('test.csv')
submission.info()

submission['processed_tweet'] = submission['tweet'].apply(process_tweet)

submission.head()

drop_features(['tweet'],submission)

submission.head()

predicted = gs.predict(submission['processed_tweet'])

predicted

final_predict = pd.DataFrame(predicted,columns=['label'])
result = pd.DataFrame(submission['id'],columns=['id'])
result = pd.concat([result,final_predict],axis=1)
result.to_csv('final_predictions.csv',index=False)

result['label'].value_counts()

